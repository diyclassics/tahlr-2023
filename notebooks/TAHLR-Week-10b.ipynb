{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAHLR Week 10b: Unsupervised Methods: Topic Modeling and Clustering\n",
    "\n",
    "Code notebook for TAHLR course at ISAW (Fall 2023) based on Albrecht et al. 2022 (Blueprints) Ch. 8: Unsupervised Methods: Topic Modeling and Clustering; streamlined version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from glob import glob\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for preprocessing\n",
    "\n",
    "import string\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, \"\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "PATH = '../data/texts/lat/'\n",
    "files = natsorted(glob(PATH + '*.txt'))\n",
    "books = [\"livy_1\", \"livy_2\", \"aen_1\", \"aen_2\", \"aen_3\", \"aen_4\", \"aen_5\", \"aen_6\", \"aen_7\", \"aen_8\", \"aen_9\", \"aen_10\", \"aen_11\", \"aen_12\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for book, file in zip(books, files):\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "        sents = sent_tokenize(text)\n",
    "        for sent in sents:\n",
    "            data.append((book, sent))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['book', 'text'])\n",
    "df['text'] = df['text'].apply(lambda x: preprocess(x))\n",
    "\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [[word for word in sent.split()] for sent in df['text']]\n",
    "words = [word for sent in words for word in sent]\n",
    "wordcounts = Counter(words)\n",
    "stopwords = [word for word, count in wordcounts.most_common(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tf-idf matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TV = TfidfVectorizer(stop_words=stopwords, max_features=10000)\n",
    "tfidf_vectors = TV.fit_transform(df['text'])\n",
    "vocab = TV.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Creating a Topic Model for Paragraphs with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider reducing number of sents (because LDA is computationally expensive)\n",
    "\n",
    "# e.g.\n",
    "# df = df[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for displaying topics\n",
    "\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, word_vector in enumerate(model.components_):\n",
    "        total = word_vector.sum()\n",
    "        largest = word_vector.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]],\n",
    "                  word_vector[largest[i]]*100.0/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose, lda_para_model; nb: could take a long time\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components = 20, random_state=42)\n",
    "W_lda_matrix = lda_model.fit_transform(tfidf_vectors)\n",
    "H_lda_matrix = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda_model, TV.get_feature_names_out(), no_top_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use pyLDAvis to visualize topics; if you can get it to work!\n",
    "\n",
    "# # !pip install pyLDAvis\n",
    "# import pyLDAvis.lda_model\n",
    "\n",
    "# lda_display = pyLDAvis.lda_model.prepare(lda_model, tfidf_vectors,\n",
    "#                             TV, sort_topics=False)\n",
    "# pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize topic \"weights\" with word cloud\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# def wordcloud_topics(model, features, no_top_words=40):\n",
    "#     for topic, words in enumerate(model.components_):\n",
    "#         size = {}\n",
    "#         largest = words.argsort()[::-1] # invert sort order\n",
    "#         for i in range(0, no_top_words):\n",
    "#             size[features[largest[i]]] = abs(words[largest[i]])\n",
    "#         wc = WordCloud(background_color=\"white\", max_words=100,\n",
    "#                        width=960, height=540)\n",
    "#         wc.generate_from_frequencies(size)\n",
    "#         plt.figure(figsize=(12,12))\n",
    "#         plt.imshow(wc, interpolation='bilinear')\n",
    "#         plt.axis(\"off\")\n",
    "#         # if you don't want to save the topic model, comment the next line\n",
    "#         plt.savefig(f'topic{topic}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud_topics(lda_model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Blueprint: Kmeans clustering w. visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7\n",
    "\n",
    "# Set up kmeans\n",
    "\n",
    "N = 3\n",
    "\n",
    "X = tfidf_vectors\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=N, random_state=42, n_init='auto')\n",
    "kmeans.fit(X)\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions with PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_vecs = pca.fit_transform(X.toarray())\n",
    "x0 = pca_vecs[:, 0]\n",
    "x1 = pca_vecs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dataframe\n",
    "\n",
    "df[\"cluster\"] = clusters\n",
    "df[\"x0\"] = x0\n",
    "df[\"x1\"] = x1\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, get top keywords\n",
    "\n",
    "def get_top_keywords(n_terms):\n",
    "    \"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
    "    df = pd.DataFrame(X.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster\n",
    "    terms = TV.get_feature_names_out() # access tf-idf terms\n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
    "            \n",
    "get_top_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map clusters to appropriate labels \n",
    "\n",
    "cluster_map = {i: f'cluster_{i}' for i in range(0, N)}\n",
    "df['cluster'] = df['cluster'].map(cluster_map)\n",
    "print(cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters with Seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.title(\"TF-IDF + KMeans on Misc. Latin\", fontdict={\"fontsize\": 18})\n",
    "\n",
    "plt.xlabel(\"X0\", fontdict={\"fontsize\": 16})\n",
    "plt.ylabel(\"X1\", fontdict={\"fontsize\": 16})\n",
    "\n",
    "label_sample = df[df['x0'] > .2].sample(5, random_state=2)\n",
    "# plot `book` at `x0` and `x1`` for each row in label_sample\n",
    "for i in range(len(label_sample)):\n",
    "    plt.text(label_sample.iloc[i]['x0'], label_sample.iloc[i]['x1'], label_sample.iloc[i]['book'], size=8)\n",
    "\n",
    "label_sample = df[df['x0'] < -.2].sample(5, random_state=2)\n",
    "# plot `book` at `x0` and `x1`` for each row in label_sample\n",
    "for i in range(len(label_sample)):\n",
    "    plt.text(label_sample.iloc[i]['x0'], label_sample.iloc[i]['x1'], label_sample.iloc[i]['book'], size=8)\n",
    "\n",
    "label_sample = df[df['x1'] < 0].sample(1, random_state=2)\n",
    "# plot `book` at `x0` and `x1`` for each row in label_sample\n",
    "for i in range(len(label_sample)):\n",
    "    plt.text(label_sample.iloc[i]['x0'], label_sample.iloc[i]['x1'], label_sample.iloc[i]['book'], size=8)\n",
    "\n",
    "sns.scatterplot(data=df, x='x0', y='x1', hue='cluster', hue_order=cluster_map.values())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
